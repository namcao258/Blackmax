<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="ProTac: Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control" name="description"/>
<meta content="Representation Learning, Tactile Sensing, Imitation Learning" name="keywords"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control</title>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"/>
<link href="./static/css/bulma.min.css" rel="stylesheet"/>
<link href="./static/css/bulma-carousel.min.css" rel="stylesheet"/>
<link href="./static/css/bulma-slider.min.css" rel="stylesheet"/>
<link href="./static/css/fontawesome.all.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet"/>
<link href="./static/css/index.css" rel="stylesheet"/>
<link href="./static/images/favicon.png" rel="icon"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer="" src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>
<style>
:root {
  --highlight: #3F72AF;
  --highlight-light: #DBE2EF;
  --highlight-dark: #112D4E;
  --text-dark: #222;
  --background: #F9F7F7;
}

.hl {
  color: var(--highlight);
  font-weight: 600;
}

a {
  color: var(--highlight);
  text-decoration: none;
}

a:hover {
  color: var(--highlight-dark);
  text-decoration: underline;
}

.button.is-dark {
  background-color: var(--highlight-dark);
  border-color: var(--highlight-dark);
}

.button.is-dark:hover {
  background-color: var(--highlight);
  border-color: var(--highlight);
}

html {
  background-color: var(--background);
  color: var(--text-dark);
}
</style><style>
h1, h2, h3, h4, h5, h6 {
  color: var(--highlight-dark);
}

.publication-title {
  color: var(--highlight-dark);
}

.title.is-1, .title.is-2, .title.is-3 {
  color: var(--highlight-dark);
}
</style></head>
<body>
<section class="hero">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control</h1>
<div class="is-size-5 publication-authors">
<span class="author-block">
<a href="https://quan-luu.github.io/">Quan Khanh Luu</a>,
            </span>
<span class="author-block">
<a href="https://scholar.google.com/citations?user=8vRPgzoAAAAJ&hl=en">Dinh Quang Nguyen</a>,
            </span>
<span class="author-block">
<a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=766&l=1">Nhan Huu Nguyen</a>,
            </span>
<span class="author-block">
<a href="https://scholar.google.com/citations?user=cEKEYFAAAAAJ&hl=en&oi=sra">Nam Phuong Dam</a>,
            </span>
<span class="author-block">
<a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=669&l=1">Van Anh Ho</a>
            </span>
</div>
<div class="is-size-5 publication-authors">
<span class="author-block">Japan Advanced Institue of Science and Technology (JAIST), Japan</span>
</div>
<div class="column has-text-centered">
<div class="publication-links">
<!-- PDF Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="files/TRO25_ProTacArm.pdf">
<span class="icon">
<i class="ai ai-arxiv"></i>
</span>
<span>Paper</span>
</a>
</span>
<!-- Video Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://youtu.be/dFgZLUpeWw4">
<span class="icon">
<i class="fab fa-youtube"></i>
</span>
<span>Video</span>
</a>
</span>
<!-- Code Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://github.com/Ho-lab-jaist/protac">
<span class="icon">
<i class="fab fa-github"></i>
</span>
<span>Code</span>
</a>
</span>
<!-- Dataset Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="">
<span class="icon">
<i class="far fa-images"></i>
</span>
<span>Data (Coming Soon)</span>
</a>
</span></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="section">
<div class="container is-max-desktop">
<!-- Abstract. -->
<div class="columns is-centered has-text-centered">
<div class="column is-four-fifths">
<h2 class="title is-3">Abstract</h2>

<div class="content has-text-centered">
<img alt="First Page Image" id="teaser" src="./static/images/Figure1a.png" width="80%"/>
<div class="content has-text-justified">
<br>
<p>
    Soft-bodied robots with multimodal sensing capabilities hold promise for versatile and user-friendly robotics.
    However, seamlessly integrating multiple sensing functionalities into soft artificial skins remains a challenge
    due to compatibility issues between soft materials and conventional electronics.

    While vision-based tactile sensing has enabled simple and effective sensor designs for robotic touch,
    there has been limited exploration of this technique for intrinsic multimodal sensing in large-sized soft robot bodies.

    To address this gap, this paper introduces a novel vision-based soft sensing technique, named ProTac,
    capable of operating either in tactile or proximity sensing modes. This vision-based sensing technology relies
    on a soft functional skin that can actively switch its optical properties between opaque and transparent states.

    Furthermore, the paper develops efficient learning pipelines for proximity and tactile perceptions,
    as well as sensing strategies enabled through the timing activation of the two sensing modes.

    The effectiveness of the soft sensing technology is demonstrated through a soft ProTac link,
    which can be integrated into newly constructed or existing commercial robot arms.

    Results suggest that robots integrated with the ProTac link, along with rigorous control formulation,
    can perform safe and purposeful control actions, enhancing human-robot interaction scenarios
    and enabling motion control tasks that are difficult with conventional rigid links.
</p>
</div>
</div>
</div>
<!--/ Abstract. -->

</div>
</section>

<!-- ProTac Section: Design Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">ProTac Design</h2>
        <p class="is-size-5 has-text-justified" style="margin-top: 1rem;">
          We introduce <span class="hl">ProTac</span>, a vision-based soft sensing link with controllable optical properties. 
          Its design integrates <span class="hl">tactile</span> and <span class="hl">proximity</span> sensing into a soft robotic link, enabling 
          <span class="hl">whole-body</span>, contact-rich, and proximity-aware manipulation.
        </p>
      </div>
    </div>

    <!-- Inspiration Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Inspiration from Electrochromic Windows</h3>
      <p class="has-text-justified">
        ProTac draws inspiration from electrochromic windows, which regulate transparency using voltage. 
        This concept enables a switchable, vision-based sensing mechanism for soft robot bodies.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="90%">
          <source src="videos/protac_inspiration.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Design Overview Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Internal Design and Construction</h3>
      <p class="has-text-justified">
        ProTac features a modular cylindrical structure composed of a soft PDLC skin, internal markers, 
        embedded cameras, and a supporting brace. This design enables effective multimodal sensing over a large surface area.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="90%">
          <source src="videos/protac_design.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Working Principle Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Working Principle: Transparency Switching</h3>
      <p class="has-text-justified">
        ProTac operates in two modes: in the <span class="hl">opaque</span> state, it detects contact across the large-area skin using a deep neural network applied to marker-featured images; 
        in the <span class="hl">transparent</span> state, it performs proximity sensing via monocular depth estimation applied to see-through images.
        This switchable mechanism enables dual-mode perception in a single soft robot body.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="90%">
          <source src="videos/protac_principle.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
</section>

<!-- ProTac Section: Perception -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">ProTac Perception</h2>

        <!-- Proximity Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Proximity Sensing</h3>
          <p class="has-text-justified">
            In its <span class="hl">transparent</span> state, ProTac estimates the proximity of nearby
            objects, providing pre-contact awareness for safe interaction.
          </p>
          <div class="has-text-centered" style="margin-top: 1rem;">
            <video autoplay muted loop controls playsinline width="90%">
              <source src="videos/proximity_sensing.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>

        <!-- Tactile Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Tactile Sensing</h3>
          <p class="has-text-justified">
            In the <span class="hl">opaque</span> state, ProTac functions as a tactile modality 
            by leveraging a sim-to-real learning pipeline to estimate contact location and intensity.
          </p>
          <div class="has-text-centered" style="margin-top: 1rem;">
            <video autoplay muted loop controls playsinline width="90%">
              <source src="videos/tactile_sensing.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>

        <!-- Flickering Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Flickering Sensing</h3>
          <p class="has-text-justified">
            The <span class="hl"><em>flickering</em></span> mode enables near-simultaneous proximity and tactile perception by rapidly
            switching between the two sensing states at a high frequency.
            This enables ProTac’s unique operational mode, providing simultaneous proximity and contact awareness.
          </p>
          <div class="has-text-centered" style="margin-top: 1rem;">
            <video autoplay muted loop controls playsinline width="90%">
              <source src="videos/flickering_sensing.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ProTac Section: Control -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">ProTac Control</h2>

        <!-- Multimodal ProTac Control -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Multimodal ProTac Control</h3>

          <!-- Adaptive Motion Control -->
          <div style="margin-top: 1.5rem;">
            <h4 class="title is-5">Adaptive Motion Control with Obstacle and Contact Awareness</h4>
            <p class="has-text-justified">
              By combining tactile and proximity sensing, ProTac delivers resilient, contact-reactive motion 
              in environments with unavoidable physical obstacles and contacts.
            </p>
            <div class="has-text-centered" style="margin-top: 1rem;">
              <video autoplay muted loop controls playsinline width="90%">
                <source src="videos/adaptive_motion_control.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Human-Robot Collaboration -->
          <div style="margin-top: 2rem;">
            <h4 class="title is-5">Multi-phase Human-Robot Interaction</h4>
            <p class="has-text-justified">
              ProTac supports multiphase human-robot interaction by distinguishing between scenarios with and without 
              physical interaction intent. This facilitates shared autonomy and close human collaboration.
            </p>

            <!-- Case 1: Human passerby -->
            <h4 class="title is-5 has-text-centered" style="margin-top: 2rem;">
              Human Passerby without Physical Interaction Intention
            </h4>
            <div class="has-text-centered" style="margin-top: 1rem;">
              <video autoplay muted loop controls playsinline width="90%">
                <source src="videos/hri_a.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Case 2: Human with intent -->
            <h4 class="title is-5 has-text-centered" style="margin-top: 2rem;">
              Human with Physical Interaction Intention
            </h4>
            <div class="has-text-centered" style="margin-top: 1rem;">
              <video autoplay muted loop controls playsinline width="90%">
                <source src="videos/hri_b.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <!-- Other ProTac Control -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Other ProTac Control Integration</h3>

          <!-- Admittance Control -->
          <div style="margin-top: 1.5rem;">
            <h4 class="title is-5">Admittance-Based Reflex Control</h4>
            <p class="has-text-justified">
              ProTac enables reflexive motion through admittance control, 
              responding flexibly to approaching obstacles and unexpected contacts.
            </p>
            <div class="has-text-centered" style="margin-top: 1rem;">
              <video autoplay muted loop controls playsinline width="90%">
                <source src="videos/admittance_control.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Speed Regulation -->
          <div style="margin-top: 2rem;">
            <h4 class="title is-5">Proximity-Aware Speed Regulation</h4>
            <p class="has-text-justified">
              By continuously estimating the distance to nearby objects, ProTac dynamically adjusts the motion speed, which is expected to ensure safe operation in shared environments.
            </p>
            <div class="has-text-centered" style="margin-top: 1rem;">
              <video autoplay muted loop controls playsinline width="90%">
                <source src="videos/speed_regulation.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End ProTac Sections -->

<!-- Related Work -->
<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column is-full-width">
<h2 class="title is-3">Related Work</h2>
  <div class="content has-text-justified">
    <p class="is-size-5 has-text-justified" style="margin-top: 1rem;">
    This work builds upon our previous proof-of-concept soft robotic link with controllable skin transparency,  
    <a class="hl" href="#protac"><strong>ProTac</strong></a>.  
    The tactile sensing pipeline is adapted from our earlier work,  
    <a class="hl" href="#simtacls"><strong>SimTacLS</strong></a>,  
    a physics-informed simulation and learning platform for large-area vision-based tactile sensors.
    </p>
  </div>

  <!-- Paper Card 1 -->
  <div id="protac" class="media" style="display: flex; align-items: center;">
    <figure class="media-left">
      <p class="image is-256x256">
        <video autoplay muted loop playsinline width="128" height="128" style="object-fit: cover;">
          <source src="https://quan-luu.github.io/tn/videos/protac_sensing.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </p>
    </figure>
    <div class="media-content">
      <p>
        <strong>Soft Robotic Link with Controllable Transparency for Vision-based Tactile and Proximity Sensing</strong><br>
        <span style="color: #3273dc;"> Quan Khanh Luu, Dinh Quang Nguyen, Nhan Huu Nguyen, Van Anh Ho</span><br>
        <span style="color: gray;">IEEE RoboSoft 2023</span><br>
        <a href="https://quan-luu.github.io/files/Soft_Robotic_Link_with_Controllable_Transparency_for_Visionbased_Tactile_and_Proximity_Sensing.pdf" target="_blank">[Paper]</a>
        <a href="https://github.com/Ho-lab-jaist/protac" target="_blank">[Code]</a>
        <a href="https://youtu.be/m8QzvDx_vpc?si=JKCnNxe6U-JkebbL" target="_blank">[Video]</a>
        <a href="https://quan-luu.github.io/files/bib/protac_robosoft.bib" target="_blank">[BibTeX]</a>
      </p>
      <p style="margin-top: 0.5rem;">
        This study presents ProTac, a novel soft robotic link with integrated tactile and proximity sensing.
      </p>
    </div>
  </div>

  <!-- Paper Card 2 -->
  <div id="simtacls" class="media" style="display: flex; align-items: center;">
    <figure class="media-left">
      <p class="image is-256x256">
        <video autoplay muted loop playsinline width="128" height="128" style="object-fit: cover;">
          <source src="https://quan-luu.github.io/tn/videos/taclink_simtacls.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </p>
    </figure>
    <div class="media-content">
      <p>
        <strong>Simulation, Learning, and Application of Vision-Based Tactile Sensing at Large Scale</strong><br>
        <span style="color: #3273dc;">Quan Khanh Luu, Nhan Huu Nguyen, Van Anh Ho</span><br>
        <span style="color: gray;">IEEE Transaction on Robotics (T-RO), 2023</span><br>
        <a href="https://quan-luu.github.io/files/Simulation_Learning_and_Application_of_Vision-Based_Tactile_Sensing_at_Large_Scale.pdf" target="_blank">[Paper]</a>
        <a href="https://github.com/Ho-lab-jaist/SimTacLS" target="_blank">[Code]</a>
        <a href="https://www.youtube.com/watch?v=NN2u8YBLITY" target="_blank">[Video]</a>
        <a href="https://quan-luu.github.io/files/bib/simtacls_tro23.bib" target="_blank">[BibTeX]</a>
      </p>
      <p style="margin-top: 0.5rem;">
        This study introduces SimTacLS, 
        a physics-informed simulation and learning pipeline designed for large-area soft vision-based tactile sensors.
      </p>
    </div>
  </div>

  <!-- Paper Card 3 -->
  <div id="taclink-control" class="media" style="display: flex; align-items: center;">
    <figure class="media-left">
      <p class="image is-256x256">
        <video autoplay muted loop playsinline width="128" height="128" style="object-fit: cover;">
          <source src="https://quan-luu.github.io/tn/videos/taclink_safety.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </p>
    </figure>
    <div class="media-content">
      <p>
        <strong>TacLink-Integrated Robot Arm toward Safe Human-Robot Interaction</strong><br>
        <span style="color: #3273dc;">Quan Khanh Luu, Alessandro Albini, Perla Maiolino, Van Anh Ho</span><br>
        <span style="color: gray;">IROS 2024</span><br>
        <a href="https://quan-luu.github.io/files/TacLink-Integrated_Robot_Arm_toward_Safe_Human-Robot_Interaction.pdf" target="_blank">[Paper]</a>
        <a href="https://github.com/Ho-lab-jaist/taclink-control" target="_blank">[Code]</a>
        <a href="https://quan-luu.github.io/videos/media/taclink_iros24.mp4" target="_blank">[Video]</a>
        <a href="https://quan-luu.github.io/files/bib/taclink_iros_luu_24.bib" target="_blank">[BibTeX]</a>
      </p>
      <p style="margin-top: 0.5rem;">
        This study explores the use of TacLink, a soft vision-based tactile link, 
        as a safety control mechanism that can potentially replace conventional rigid robot links and impact observers.
      </p>
    </div>
  </div>

  <!-- Paper Card 3 -->
  <div id="vi2tap" class="media" style="display: flex; align-items: center;">
    <figure class="media-left">
      <p class="image is-256x256">
        <video autoplay muted loop playsinline width="128" height="128" style="object-fit: cover;">
          <source src="https://quan-luu.github.io/tn/videos/vi2tap.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </p>
    </figure>
    <div class="media-content">
      <p>
        <strong>Vi2TaP: A Cross-Polarization Based Mechanism for Perception Transition in Tactile-Proximity Sensing with Applications to Soft Grippers</strong><br>
        <span style="color: #3273dc;"> Nhan Huu Nguyen, Nhat Minh Dinh Le, Quan Khanh Luu, Tuan Tai Nguyen, Van Anh Ho</span><br>
        <span style="color: gray;">IEEE Robotics and Automation Letters (RA-L), 2025 </span><br>
        <a href="https://quan-luu.github.io/files/Vi2TaP.pdf" target="_blank">[Paper]</a>
        <a href="https://ieeexplore.ieee.org/document/10982157/media#media" target="_blank">[Video]</a>
        <a href="https://quan-luu.github.io/files/bib/vi2tap_ral.bib" target="_blank">[BibTeX]</a>
      </p>
      <p style="margin-top: 0.5rem;">
        This study introduces Vi2TaP, a cross-polarization-based multimodal soft gripper that seamlessly 
        switches between tactile and proximity sensing.
      </p>
    </div>
  </div>

</div>
</div>
</div>
</section>
<!-- End Related Work -->

<!-- BibTeX -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column is-full-width">
<h2 class="title is-3">BibTeX</h2>
    <!-- <pre><code>
@misc{luu2025manifeelbenchmarkingunderstandingvisuotactile,
  title        = {ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning},
  author       = {Quan Khanh Luu and Pokuang Zhou and Zhengtong Xu and Zhiyuan Zhang and Qiang Qiu and Yu She},
  year         = {2025},
  eprint       = {2505.18472},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url          = {https://arxiv.org/abs/2505.18472}
}
    </code></pre> -->
</div>
</div>
</div>
</section>
<!-- End BibTeX -->

<footer class="footer">
<div class="container">
<div class="content has-text-centered">
<a class="external-link" disabled="" href="https://github.com/quan-luu">
<i class="fab fa-github"></i>
</a>
</div>
<div class="columns is-centered">
<div class="column is-8">
<div class="content">
<p>
            This website is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
<p>
            The template of this website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
</div>
</div>
</div>
</div>
</footer>

</body>
</html>