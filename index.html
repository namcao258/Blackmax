<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="ProTac: Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control" name="description"/>
<meta content="Representation Learning, Tactile Sensing, Imitation Learning" name="keywords"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control</title>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"/>
<link href="./static/css/bulma.min.css" rel="stylesheet"/>
<link href="./static/css/bulma-carousel.min.css" rel="stylesheet"/>
<link href="./static/css/bulma-slider.min.css" rel="stylesheet"/>
<link href="./static/css/fontawesome.all.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet"/>
<link href="./static/css/index.css" rel="stylesheet"/>
<link href="./static/images/favicon.png" rel="icon"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer="" src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>
<style>
:root {
  --highlight: #3F72AF;
  --highlight-light: #DBE2EF;
  --highlight-dark: #112D4E;
  --text-dark: #222;
  --background: #F9F7F7;
}

.hl {
  color: var(--highlight);
  font-weight: 600;
}

a {
  color: var(--highlight);
  text-decoration: none;
}

a:hover {
  color: var(--highlight-dark);
  text-decoration: underline;
}

.button.is-dark {
  background-color: var(--highlight-dark);
  border-color: var(--highlight-dark);
}

.button.is-dark:hover {
  background-color: var(--highlight);
  border-color: var(--highlight);
}

html {
  background-color: var(--background);
  color: var(--text-dark);
}
</style><style>
h1, h2, h3, h4, h5, h6 {
  color: var(--highlight-dark);
}

.publication-title {
  color: var(--highlight-dark);
}

.title.is-1, .title.is-2, .title.is-3 {
  color: var(--highlight-dark);
}
</style></head>
<body>
<section class="hero">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">Vision-based Proximity and Tactile Sensing for Robot Arms: Design, Perception, and Control</h1>
<div class="is-size-5 publication-authors">
<span class="author-block">
<a href="https://quan-luu.github.io/">Quan Khanh Luu</a>,
            </span>
<span class="author-block">
<a href="https://scholar.google.com/citations?user=8vRPgzoAAAAJ&hl=en">Dinh Quang Nguyen</a>,
            </span>
<span class="author-block">
<a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=766&l=1">Nhan Huu Nguyen</a>,
            </span>
<span class="author-block">
<a href="https://scholar.google.com/citations?user=cEKEYFAAAAAJ&hl=en&oi=sra">Nam Phuong Dam</a>,
            </span>
<span class="author-block">
<a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=669&l=1">Van Anh Ho</a>
            </span>
</div>
<div class="is-size-5 publication-authors">
<span class="author-block">Japan Advanced Institue of Science and Technology (JAIST), Japan</span>
</div>
<div class="column has-text-centered">
<div class="publication-links">
<!-- PDF Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="">
<span class="icon">
<i class="ai ai-arxiv"></i>
</span>
<span>Paper</span>
</a>
</span>
<!-- Video Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://www.youtube.com/watch?v=5DhAhlTVxzg">
<span class="icon">
<i class="fab fa-youtube"></i>
</span>
<span>Video</span>
</a>
</span>
<!-- Code Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://github.com/Ho-lab-jaist/protac">
<span class="icon">
<i class="fab fa-github"></i>
</span>
<span>Code</span>
</a>
</span>
<!-- Dataset Link. -->
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="">
<span class="icon">
<i class="far fa-images"></i>
</span>
<span>Data (Coming Soon)</span>
</a>
</span></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="section">
<div class="container is-max-desktop">
<!-- Abstract. -->
<div class="columns is-centered has-text-centered">
<div class="column is-four-fifths">
<h2 class="title is-3">Abstract</h2>

<div class="content has-text-centered">
<img alt="First Page Image" id="teaser" src="./static/images/Figure1a.png" width="80%"/>
<div class="content has-text-justified">
<br>
<p>
    Soft-bodied robots with multimodal sensing capabilities hold promise for versatile and user-friendly robotics.
    However, seamlessly integrating multiple sensing functionalities into soft artificial skins remains a challenge
    due to compatibility issues between soft materials and conventional electronics.

    While vision-based tactile sensing has enabled simple and effective sensor designs for robotic touch,
    there has been limited exploration of this technique for intrinsic multimodal sensing in large-sized soft robot bodies.

    To address this gap, this paper introduces a novel vision-based soft sensing technique, named ProTac,
    capable of operating either in tactile or proximity sensing modes. This vision-based sensing technology relies
    on a soft functional skin that can actively switch its optical properties between opaque and transparent states.

    Furthermore, the paper develops efficient learning pipelines for proximity and tactile perceptions,
    as well as sensing strategies enabled through the timing activation of the two sensing modes.

    The effectiveness of the soft sensing technology is demonstrated through a soft ProTac link,
    which can be integrated into newly constructed or existing commercial robot arms.

    Results suggest that robots integrated with the ProTac link, along with rigorous control formulation,
    can perform safe and purposeful control actions, enhancing human-robot interaction scenarios
    and enabling motion control tasks that are difficult with conventional rigid links.
</p>
</div>
</div>
</div>
<!--/ Abstract. -->

</div>
</section>

<!-- ProTac Section: Design Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">ProTac Design</h2>
        <p class="is-size-5 has-text-justified" style="margin-top: 1rem;">
          We introduce <span class="hl">ProTac</span>, a vision-based soft sensing link with controllable optical properties. 
          Its design integrates <span class="hl">tactile</span> and <span class="hl">proximity</span> sensing into a soft robotic link, enabling 
          <span class="hl">whole-body</span>, contact-rich, and proximity-aware manipulation.
        </p>
      </div>
    </div>

    <!-- Inspiration Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Inspiration from Electrochromic Windows</h3>
      <p class="has-text-justified">
        ProTac draws inspiration from electrochromic windows, which regulate transparency using voltage. 
        This concept enables a switchable, vision-based sensing mechanism for the soft robotic link.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="100%">
          <source src="/videos/protac_inspiration.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Design Overview Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Internal Design and Construction</h3>
      <p class="has-text-justified">
        ProTac features a modular cylindrical structure composed of a soft PDLC skin, internal markers, 
        an embedded camera, and a supporting brace. This design enables robust multimodal sensing over a large surface area.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="90%">
          <source src="/videos/protac_design.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Working Principle Video -->
    <div class="box" style="margin-top: 2rem;">
      <h3 class="title is-4">Working Principle: Transparency Switching</h3>
      <p class="has-text-justified">
        ProTac operates in two modes: in the opaque state, it detects contact through vision-based deformation tracking; 
        in the transparent state, it performs proximity sensing using light scattering and depth estimation. 
        This switchable mechanism enables dual-mode perception in a single soft sensing link.
      </p>
      <div class="has-text-centered" style="margin-top: 1rem;">
        <video autoplay muted loop controls playsinline width="90%">
          <source src="/videos/protac_principle.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
</section>

<!-- ProTac Section: Perception -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">ProTac Perception</h2>

        <!-- Proximity Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Proximity Sensing</h3>
          <p class="has-text-justified">
            ProTac leverages light scattering in its transparent mode to detect nearby objects, supporting pre-contact awareness.
          </p>
          <!-- Optional video or image here -->
        </div>

        <!-- Tactile Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Tactile Sensing</h3>
          <p class="has-text-justified">
            In its opaque mode, ProTac functions as a tactile sensor using vision-based deformation tracking of the internal gel layer.
          </p>
        </div>

        <!-- Flickering Sensing -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Flickering Sensing</h3>
          <p class="has-text-justified">
            ProTac can rapidly toggle between modes to achieve a hybrid flickering sensing regime, optimizing responsiveness and resolution.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ProTac Section: Control -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">ProTac Control</h2>

        <!-- Adaptive Motion Control -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Adaptive Motion Control with Obstacle and Contact Awareness</h3>
          <p class="has-text-justified">
            By incorporating proximity and tactile feedback, ProTac enables smooth, reactive motion control in constrained environments.
          </p>
        </div>

        <!-- Human-Robot Collaboration -->
        <div class="box" style="margin-top: 2rem;">
          <h3 class="title is-4">Multiphase Human-Robot Collaboration with Multimodal Sensing</h3>
          <p class="has-text-justified">
            ProTac facilitates nuanced interactions in collaborative scenarios, supporting role transitions and shared autonomy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End ProTac Sections -->

<!-- Related Work -->
<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column is-full-width">
<h2 class="title is-3">Related Work</h2>
<div class="content has-text-justified">
  <p class="is-size-5 has-text-justified" style="margin-top: 1rem;">
  This work builds upon our previous proof-of-concept soft robotic link with controllable skin transparency,  
  <a class="hl" href="https://quan-luu.github.io/files/Soft_Robotic_Link_with_Controllable_Transparency_for_Visionbased_Tactile_and_Proximity_Sensing.pdf" target="_blank"><strong>ProTac</strong></a>.  
  The tactile sensing pipeline is adapted from our earlier work,  
  <a class="hl" href="https://quan-luu.github.io/files/Simulation_Learning_and_Application_of_Vision-Based_Tactile_Sensing_at_Large_Scale.pdf" target="_blank"><strong>SimTacLS</strong></a>,  
  a physics-informed simulation and learning platform for large-area vision-based tactile sensors.
  </p>
</div>
</div>
</div>
</div>
</section>
<!-- End Related Work -->

<!-- BibTeX -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column is-full-width">
<h2 class="title is-3">BibTeX</h2>
    <!-- <pre><code>
@misc{luu2025manifeelbenchmarkingunderstandingvisuotactile,
  title        = {ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning},
  author       = {Quan Khanh Luu and Pokuang Zhou and Zhengtong Xu and Zhiyuan Zhang and Qiang Qiu and Yu She},
  year         = {2025},
  eprint       = {2505.18472},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url          = {https://arxiv.org/abs/2505.18472}
}
    </code></pre> -->
</div>
</div>
</div>
</section>
<!-- End BibTeX -->

<footer class="footer">
<div class="container">
<div class="content has-text-centered">
<a class="external-link" disabled="" href="https://github.com/quan-luu">
<i class="fab fa-github"></i>
</a>
</div>
<div class="columns is-centered">
<div class="column is-8">
<div class="content">
<p>
            This website is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
<p>
            The template of this website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
</div>
</div>
</div>
</div>
</footer>

</body>
</html>